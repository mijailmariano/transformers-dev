{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 18:39:18.186626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# load data\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# KerasNLP Training Approach\n",
    "\n",
    "### Using the KerasNLP API to train transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import 'GLUE' dataset: General Language Understanding Evaluation benchmark\n",
    "# import 'CoLA' dataset: Corpus of Linguistic Acceptability\n",
    "# documentation: https://huggingface.co/datasets/glue\n",
    "\n",
    "dataset = load_dataset('glue', 'cola')\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': [\"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       "  \"One more pseudo generalization and I'm giving up.\",\n",
       "  \"One more pseudo generalization or I'm giving up.\",\n",
       "  'The more we study verbs, the crazier they get.',\n",
       "  'Day by day the facts are getting murkier.',\n",
       "  \"I'll fix you a drink.\",\n",
       "  'Fred watered the plants flat.',\n",
       "  'Bill coughed his way out of the restaurant.',\n",
       "  \"We're dancing the night away.\",\n",
       "  'Herman hammered the metal flat.',\n",
       "  'The critics laughed the play off the stage.',\n",
       "  'The pond froze solid.',\n",
       "  'Bill rolled out of the room.',\n",
       "  'The gardener watered the flowers flat.',\n",
       "  'The gardener watered the flowers.',\n",
       "  'Bill broke the bathtub into pieces.',\n",
       "  'Bill broke the bathtub.',\n",
       "  'They drank the pub dry.',\n",
       "  'They drank the pub.',\n",
       "  'The professor talked us into a stupor.'],\n",
       " 'label': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the training data\n",
    "\n",
    "dataset[\"train\"][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only taking the training split\n",
    "\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loading a tokenizing the data as NumPy arrays\\n    Notes: \\n        - labels consists of lists of 1s and 0s\\n        - this can be converted to a NumPy array w.o. tokenizing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''loading a tokenizing the data as NumPy arrays\n",
    "    Notes: \n",
    "        - labels consists of lists of 1s and 0s\n",
    "        - this can be converted to a NumPy array w.o. tokenizing'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "setting tokenizer parameters\n",
    "    Args:\n",
    "        dataset[\"sentence\"]: The text data to be tokenized, where each element is a string.\n",
    "        return_tensors=\"np\": Specifies the output format as NumPy arrays.\n",
    "        padding=True: Ensures all tokenized outputs are of the same length by adding padding.\n",
    "'''\n",
    "\n",
    "tokenized_data = tokenizer(\n",
    "    dataset[\"sentence\"], \n",
    "    return_tensors=\"np\", \n",
    "    padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "- \"BatchEncoding\" here neatly organizes all the information. \n",
    "- We can think of it like a container or a table where each row represents a piece of text from your batch, and each column contains the different pieces of information (like token IDs, attention masks) for that text.\n",
    "'''\n",
    "\n",
    "tokenized_data = dict(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label field is already an array of 0s and 1s\n",
    "# convert the labels to a NumPy array\n",
    "\n",
    "labels = np.array(dataset[\"label\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f56dab65dbc48f4bb3c7f718eab3f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# now we load, compile, and fit the model\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower learning rates are often better for fine-tuning transformers\n",
    "\n",
    "model.compile(optimizer=Adam(3e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 1591s 6s/step - loss: 0.4887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1679d9810>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the tokenized data\n",
    "\n",
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
